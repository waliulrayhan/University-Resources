{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc02c4ef-c67c-49f3-98db-3d35a1e180cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n"
     ]
    }
   ],
   "source": [
    "# 6.1 Logistic Regression\n",
    "# Logistic Regression is used when the target variable is categorical (e.g., yes/no or 0/1). Let’s start with logistic regression to predict if a person has diabetes based on several health metrics.\n",
    "\n",
    "# 1. Loading the Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the diabetes dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b05ecb-5fb5-4b7a-bb38-3937cd324f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n",
      "Training data shape: (614, 8)\n",
      "Testing data shape: (154, 8)\n"
     ]
    }
   ],
   "source": [
    "# 2. Exploring and Preparing the Data\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b09aaa-8518-4140-9216-10612ce95e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Coefficients: [[ 0.06437138  0.03409553 -0.01387889  0.00329279 -0.0018035   0.10260248\n",
      "   0.62671121  0.03709787]]\n"
     ]
    }
   ],
   "source": [
    "# 3. Training the Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Output model coefficients\n",
    "print(\"Model Coefficients:\", model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127fc4f4-e174-4695-bb5b-d9fd51ef6874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Actual  Predicted\n",
      "668       0          0\n",
      "324       0          0\n",
      "624       0          0\n",
      "690       0          0\n",
      "473       0          0\n"
     ]
    }
   ],
   "source": [
    "# 4. Making Predictions\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compare actual vs predicted values\n",
    "comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "print(comparison.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "527e2f43-e06d-4834-993a-2948b69bb32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.68%\n",
      "Confusion Matrix:\n",
      "[[78 21]\n",
      " [18 37]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80        99\n",
      "           1       0.64      0.67      0.65        55\n",
      "\n",
      "    accuracy                           0.75       154\n",
      "   macro avg       0.73      0.73      0.73       154\n",
      "weighted avg       0.75      0.75      0.75       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluating the Model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225d1526-15e0-419b-8908-62176a885150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 75.97402597402598\n",
      "Confusion Matrix:\n",
      "[[76 23]\n",
      " [14 41]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.77      0.80        99\n",
      "           1       0.64      0.75      0.69        55\n",
      "\n",
      "    accuracy                           0.76       154\n",
      "   macro avg       0.74      0.76      0.75       154\n",
      "weighted avg       0.77      0.76      0.76       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Decision Trees\n",
    "# Now that you’ve seen logistic regression, let’s move on to Decision Trees, which are great for classification problems and provide easy-to-interpret decision rules.\n",
    "\n",
    "# Training a Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt) * 100)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "584ca28d-ab8d-4c11-8ce7-5b5bb278e028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 72.07792207792207\n",
      "Confusion Matrix:\n",
      "[[77 22]\n",
      " [21 34]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78        99\n",
      "           1       0.61      0.62      0.61        55\n",
      "\n",
      "    accuracy                           0.72       154\n",
      "   macro avg       0.70      0.70      0.70       154\n",
      "weighted avg       0.72      0.72      0.72       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.3 Random Forest\n",
    "# A Random Forest is an ensemble of decision trees, and it usually performs better than a single decision tree because it reduces overfitting.\n",
    "\n",
    "# Training a Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf) * 100)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a090b64-8c8e-4a21-a538-1bee48388de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
